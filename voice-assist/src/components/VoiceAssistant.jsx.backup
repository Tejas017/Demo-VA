import React, { useEffect, useRef, useState } from "react";
import { useVoiceAssistant } from "../contexts/VoiceAssistantContext";

// HIPAA-compliant voice assistant with wake word detection and local Whisper STT
// Flow: Wake word (Web Speech) â†’ Whisper transcription â†’ Hotkey pause â†’ Back to wake word

export default function VoiceAssistant() {
  const {
    listening,
    setListening,
    wakeWord,
    hotkey,
    sttProvider, // Should be 'whisper'
  } = useVoiceAssistant();

  // Privacy notice state - shown when user first clicks mic button
  const [showPrivacy, setShowPrivacy] = useState(false);
  const [privacyAccepted, setPrivacyAccepted] = useState(false);

  // Transcription state
  const [transcript, setTranscript] = useState("");
  const [isWaitingForWakeWord, setIsWaitingForWakeWord] = useState(true);

  // Whisper audio stream refs
  const mediaStreamRef = useRef(null);
  const mediaRecorderRef = useRef(null);
  const audioChunksRef = useRef([]);
  const recordingTimeoutRef = useRef(null);

  // Wake word recognition (Web Speech API)
  const wakeRecognitionRef = useRef(null);
  const wakeRunningRef = useRef(false);

  // State management
  const isWaitingForWakeWordRef = useRef(true);
  const listeningRef = useRef(false);
  const keyPressedRef = useRef(false);

  // Hotkey toggle (pause/resume STT only; mic can remain open in future when we add getUserMedia)
  useEffect(() => {
    const handleKeyDown = (e) => {
      if (e.key.toUpperCase() === hotkey && !keyPressedRef.current) {
        keyPressedRef.current = true;
        pausedRef.current = !pausedRef.current;
        setPaused(pausedRef.current);
        console.log(
          pausedRef.current
            ? `Mic paused (${hotkey})`
            : `Mic resumed (${hotkey})`
        );
      }
    };
    const handleKeyUp = (e) => {
      if (e.key.toUpperCase() === hotkey) keyPressedRef.current = false;
    };
    window.addEventListener("keydown", handleKeyDown);
    window.addEventListener("keyup", handleKeyUp);
    return () => {
      window.removeEventListener("keydown", handleKeyDown);
      window.removeEventListener("keyup", handleKeyUp);
    };
  }, [hotkey, setPaused]);

  // Keep pausedRef in sync
  useEffect(() => {
    pausedRef.current = paused;
  }, [paused]);

  // Store latest values in refs so event handlers can access them without recreating recognizers
  const wakeWordRef = useRef(wakeWord);
  const listeningRef = useRef(listening);
  const resumeViaWakeWordRef = useRef(resumeViaWakeWord);
  const wakeWordModeRef = useRef(wakeWordMode);

  useEffect(() => {
    wakeWordRef.current = wakeWord;
  }, [wakeWord]);

  useEffect(() => {
    listeningRef.current = listening;
  }, [listening]);

  useEffect(() => {
    resumeViaWakeWordRef.current = resumeViaWakeWord;
  }, [resumeViaWakeWord]);

  useEffect(() => {
    wakeWordModeRef.current = wakeWordMode;
  }, [wakeWordMode]);

  // Initialize Web Speech instances ONCE when component mounts
  useEffect(() => {
    if (!("webkitSpeechRecognition" in window)) {
      console.warn("Speech Recognition not supported in this browser.");
      return;
    }

    const SpeechRecognition =
      window.SpeechRecognition || window.webkitSpeechRecognition;

    // STT recognizer: used when provider === 'webspeech' and not paused
    if (!sttRecognitionRef.current) {
      const stt = new SpeechRecognition();
      stt.lang = "en-US";
      stt.continuous = true;
      stt.interimResults = false; // final results only
      stt.maxAlternatives = 1;

      stt.onstart = () => {
        sttRunningRef.current = true;
        console.log("ðŸŽ™ï¸ STT recognition started (webspeech)");
      };

      stt.onend = () => {
        sttRunningRef.current = false;
        if (sttActiveRef.current) {
          setTimeout(() => {
            if (!sttRunningRef.current) stt.start();
          }, 500);
        }
      };

      stt.onresult = (event) => {
        const transcript =
          event.results[event.results.length - 1][0].transcript;
        const normalized = transcript.trim().toLowerCase();
        const normalizedWake = wakeWordRef.current.trim().toLowerCase();

        if (pausedRef.current) return; // gated by paused

        console.log("ðŸŽ¤ Heard (STT):", transcript);

        // Optional voice controls
        if (normalized.includes("stop listening")) {
          pausedRef.current = true;
          setPaused(true);
          return;
        }
        if (normalized.includes("resume listening")) {
          pausedRef.current = false;
          setPaused(false);
          return;
        }

        if (normalized.includes(normalizedWake)) {
          console.log(
            `âœ… Wake word detected (within STT): "${wakeWordRef.current}"`
          );
        }
      };

      stt.onerror = (event) => {
        console.error("STT recognition error:", event.error);
        sttRunningRef.current = false;
        if (
          sttActiveRef.current &&
          !["aborted", "not-allowed", "service-not-allowed"].includes(
            event.error
          )
        ) {
          setTimeout(() => {
            if (!sttRunningRef.current) stt.start();
          }, 1000);
        }
      };

      sttRecognitionRef.current = stt;
    }

    // Wake-word recognizer: used when wakeWordMode === 'webspeech' while paused
    if (!wakeRecognitionRef.current) {
      const wakeRec = new SpeechRecognition();
      wakeRec.lang = "en-US";
      wakeRec.continuous = true;
      wakeRec.interimResults = true; // get partials to react faster
      wakeRec.maxAlternatives = 1;

      wakeRec.onstart = () => {
        wakeRunningRef.current = true;
        console.log("ðŸ›Žï¸ Wake recognition started (webspeech)");
      };
      wakeRec.onend = () => {
        wakeRunningRef.current = false;
        // Auto-restart if we still want wake detection
        const shouldBeRunning =
          listeningRef.current &&
          pausedRef.current &&
          resumeViaWakeWordRef.current &&
          wakeWordModeRef.current === "webspeech";
        if (shouldBeRunning) {
          setTimeout(() => {
            if (!wakeRunningRef.current) wakeRec.start();
          }, 500);
        }
      };
      wakeRec.onresult = (event) => {
        const transcript =
          event.results[event.results.length - 1][0].transcript;
        const normalized = transcript.trim().toLowerCase();
        const normalizedWake = wakeWordRef.current.trim().toLowerCase();
        if (normalized.includes(normalizedWake)) {
          console.log(
            `âœ… Wake word detected (wake listener): "${wakeWordRef.current}"`
          );
          // Resume STT
          pausedRef.current = false;
          setPaused(false);
          try {
            wakeRec.stop(); // we'll switch to STT recognizer or paid stream
          } catch {}
        }
      };
      wakeRec.onerror = (event) => {
        console.error("Wake recognition error:", event.error);
        wakeRunningRef.current = false;
        const shouldBeRunning =
          listeningRef.current &&
          pausedRef.current &&
          resumeViaWakeWordRef.current &&
          wakeWordModeRef.current === "webspeech";
        if (
          shouldBeRunning &&
          !["aborted", "not-allowed", "service-not-allowed"].includes(
            event.error
          )
        ) {
          setTimeout(() => {
            if (!wakeRunningRef.current) wakeRec.start();
          }, 1000);
        }
      };
      wakeRecognitionRef.current = wakeRec;
    }

    // Cleanup on unmount
    return () => {
      if (sttRecognitionRef.current) {
        try {
          sttRecognitionRef.current.stop();
        } catch {}
      }
      if (wakeRecognitionRef.current) {
        try {
          wakeRecognitionRef.current.stop();
        } catch {}
      }
    };
  }, []); // Empty deps - only run once on mount

  // Paid STT placeholders (start/stop). Replace with real streaming integration later.
  const paidStreamRef = useRef(null);
  const startPaidStream = () => {
    if (paidStreamRef.current) return;
    console.log("ðŸŒ Starting paid STT stream (placeholder)");
    paidStreamRef.current = { startedAt: Date.now() };
  };
  const stopPaidStream = () => {
    if (!paidStreamRef.current) return;
    console.log("ðŸ›‘ Stopping paid STT stream (placeholder)");
    paidStreamRef.current = null;
  };

  // Control which recognizers/streams are active based on state
  useEffect(() => {
    const stt = sttRecognitionRef.current;
    const wakeRec = wakeRecognitionRef.current;

    const wantSTT = listening && !paused; // transcription phase
    const wantWake = listening && paused && resumeViaWakeWord;

    // HIPAA: Whisper STT provider
    if (sttProvider === "whisper") {
      if (wantSTT) {
        // Start local audio capture and send to backend
        if (!mediaStreamRef.current) {
          console.log("ðŸŽ¤ Starting Whisper audio capture...");
          navigator.mediaDevices
            .getUserMedia({ audio: true })
            .then((stream) => {
              console.log("âœ… Microphone access granted");
              mediaStreamRef.current = stream;

              // Use audio/webm as it's widely supported
              const options = { mimeType: "audio/webm" };
              mediaRecorderRef.current = new window.MediaRecorder(
                stream,
                options
              );
              audioChunksRef.current = [];

              mediaRecorderRef.current.ondataavailable = (e) => {
                if (e.data.size > 0) {
                  console.log("ðŸ“¦ Audio chunk received:", e.data.size, "bytes");
                  audioChunksRef.current.push(e.data);
                }
              };

              mediaRecorderRef.current.onstop = async () => {
                // HIPAA: Send audio securely to backend
                const audioBlob = new Blob(audioChunksRef.current, {
                  type: "audio/webm",
                });
                audioChunksRef.current = [];

                console.log(
                  "ðŸŽ™ï¸ Audio captured, size:",
                  audioBlob.size,
                  "bytes"
                );

                try {
                  const formData = new FormData();
                  formData.append("audio", audioBlob, "recording.webm");

                  console.log("ðŸ“¤ Sending audio to Whisper backend...");

                  const response = await fetch(
                    "http://localhost:5000/api/whisper-transcribe",
                    {
                      method: "POST",
                      body: formData,
                    }
                  );

                  if (response.ok) {
                    const data = await response.json();
                    const transcriptText = data.transcript || "";
                    console.log("âœ… Transcription received:", transcriptText);
                    setTranscript(transcriptText);
                  } else {
                    const errorText = await response.text();
                    console.error(
                      "âŒ Transcription failed:",
                      response.status,
                      errorText
                    );
                    setTranscript("Transcription failed.");
                  }
                } catch (error) {
                  console.error("âŒ Transcription error:", error);
                  setTranscript("Transcription error.");
                }
              };

              mediaRecorderRef.current.onstart = () => {
                console.log("ðŸ”´ Recording started");
              };

              mediaRecorderRef.current.onerror = (e) => {
                console.error("âŒ MediaRecorder error:", e);
              };

              // Start recording
              console.log("ðŸŽ¬ Starting MediaRecorder...");
              mediaRecorderRef.current.start();

              // Auto-stop after 5 seconds to get transcription
              setTimeout(() => {
                if (
                  mediaRecorderRef.current &&
                  mediaRecorderRef.current.state === "recording"
                ) {
                  console.log("â¹ï¸ Auto-stopping recording after 5 seconds");
                  mediaRecorderRef.current.stop();
                }
              }, 5000);
            })
            .catch((error) => {
              console.error("âŒ Microphone access denied:", error);
              setTranscript("Microphone access denied.");
            });
        } else if (
          mediaRecorderRef.current &&
          mediaRecorderRef.current.state === "inactive"
        ) {
          console.log("â–¶ï¸ Restarting recording...");
          audioChunksRef.current = [];
          mediaRecorderRef.current.start();

          // Auto-stop after 5 seconds
          setTimeout(() => {
            if (
              mediaRecorderRef.current &&
              mediaRecorderRef.current.state === "recording"
            ) {
              console.log("â¹ï¸ Auto-stopping recording after 5 seconds");
              mediaRecorderRef.current.stop();
            }
          }, 5000);
        }
      } else {
        // Stop recording
        console.log("â¸ï¸ Stopping Whisper recording...");
        if (
          mediaRecorderRef.current &&
          mediaRecorderRef.current.state === "recording"
        ) {
          mediaRecorderRef.current.stop();
        }
        if (mediaStreamRef.current) {
          mediaStreamRef.current.getTracks().forEach((track) => track.stop());
          mediaStreamRef.current = null;
          console.log("ðŸ›‘ Microphone stream closed");
        }
      }
      // No browser STT or paid STT
      return;
    }

    // ...existing code for webspeech and paid...
    if (sttProvider === "webspeech") {
      // ...existing code...
    } else if (sttProvider === "paid") {
      // ...existing code...
    }

    // Cleanup when component unmounts
    return () => {
      // HIPAA: Stop audio tracks and recorder
      if (
        mediaRecorderRef.current &&
        mediaRecorderRef.current.state === "recording"
      ) {
        mediaRecorderRef.current.stop();
      }
      if (mediaStreamRef.current) {
        mediaStreamRef.current.getTracks().forEach((track) => track.stop());
        mediaStreamRef.current = null;
      }
    };
  }, [listening, paused, resumeViaWakeWord, sttProvider, wakeWordMode]);

  // HIPAA: Privacy notice UI
  if (showPrivacy) {
    return (
      <div
        className="hipaa-privacy-notice"
        style={{
          position: "fixed",
          top: 0,
          left: 0,
          width: "100vw",
          height: "100vh",
          background: "rgba(30,64,175,0.08)",
          zIndex: 9999,
          display: "flex",
          alignItems: "center",
          justifyContent: "center",
          flexDirection: "column",
        }}
      >
        <div
          style={{
            background: "#fff",
            padding: "2rem 2.5rem",
            borderRadius: 16,
            boxShadow: "0 4px 24px rgba(30,64,175,0.12)",
            maxWidth: 420,
          }}
        >
          <h2 style={{ color: "#1976d2", marginBottom: 12 }}>Privacy Notice</h2>
          <p style={{ color: "#333", fontSize: "1.1rem", marginBottom: 18 }}>
            This voice assistant is HIPAA-compliant. Your audio is processed
            securely and never sent to third-party cloud providers.
            Transcriptions are handled only on trusted infrastructure. No PHI is
            stored or exposed in browser logs.
          </p>
          <button
            style={{
              background: "#1976d2",
              color: "#fff",
              border: "none",
              borderRadius: 8,
              padding: "0.7rem 1.5rem",
              fontWeight: 600,
              fontSize: "1rem",
              cursor: "pointer",
            }}
            onClick={() => setShowPrivacy(false)}
          >
            I Understand
          </button>
        </div>
      </div>
    );
  }

  // HIPAA: Show transcript securely
  if (sttProvider === "whisper") {
    return (
      <div
        style={{
          position: "fixed",
          bottom: 24,
          left: 24,
          background: "#fff",
          borderRadius: 12,
          boxShadow: "0 2px 12px rgba(30,64,175,0.10)",
          padding: "1rem 1.5rem",
          zIndex: 1000,
          maxWidth: 420,
        }}
      >
        <h4 style={{ color: "#1976d2", marginBottom: 8 }}>Transcription</h4>
        <div
          style={{ color: "#222", fontSize: "1.1rem", wordBreak: "break-word" }}
        >
          {transcript}
        </div>
      </div>
    );
  }
  return null;
}
